{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 플랫폼 업로드를 쉽게하기 위한 로컬 개발 코드\n",
    "- T3Q.ai(T3Q.cep + T3Q.dl): 빅데이터/인공지능 통합 플랫폼\n",
    "- 플랫폼 업로드를 쉽게하기 위하여 로컬에서 아래의 코드(파일1)를 개발한다.\n",
    "- 파일 1(파일명): 1_local_platform_image_classification.ipynb\n",
    "\n",
    "### 전처리 객체 또는 학습모델 객체\n",
    "- 전처리 객체나 학습모델 객체는 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "### 데이터셋 (학습 데이터/테스트 데이터)\n",
    "- 학습과 테스트에 사용되는 데이터를 나누어 관리한다.\n",
    "- 학습 데이터: dataset 폴더 아래에 저장하거나 dataset.zip 파일 형태로 저장한다.\n",
    "- 테스트 데이터: test_dataset 폴더 아래에 저장하거나 test_dataset.zip 파일 형태로 저장한다.\n",
    "\n",
    "### 로컬 개발 워크플로우(workflow)  \n",
    "- 로컬 개발 워크플로우를 다음의 4단계로 분리한다.\n",
    "\n",
    "1. 데이터셋 준비(Data Setup)\n",
    "- 로컬 저장소에서 전처리 및 학습에 필요한 학습 데이터셋을 준비한다.\n",
    "\n",
    "2. 데이터 전처리(Data Preprocessing)\n",
    "- 데이터셋의 분석 및 정규화(Normalization)등의 전처리를 수행한다.\n",
    "- 데이터를 모델 학습에 사용할 수 있도록 가공한다.\n",
    "- 추론과정에서 필요한 경우, 데이터 전처리에 사용된 객체를 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "3. 학습 모델 훈련(Train Model)\n",
    "- 데이터를 훈련에 사용할 수 있도록 가공한 뒤에 학습 모델을 구성한다. \n",
    "- 학습 모델을 준비된 데이터셋으로 훈련시킨다.\n",
    "- 정확도(Accuracy)나 손실(Loss)등 학습 모델의 성능을 검증한다.\n",
    "- 학습 모델의 성능 검증 후, 학습 모델을 배포한다.\n",
    "- 배포할 학습 모델을 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "4. 추론(Inference)\n",
    "- 저장된 전처리 객체나 학습 모델 객체를 준비한다.\n",
    "- 추론에 필요한 테스트 데이터셋을 준비한다.\n",
    "- 배포된 학습 모델을 통해 테스트 데이터에 대한 추론을 진행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공지능 통합플랫폼(T3Q.ai) 프로세스를 이해하고 인공지능 쉽게 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 머신러닝(Machine Learning)과 딥러닝(Deep Learning) 프로그래밍 패턴\n",
    "\n",
    "(1) 데이터셋 불러오기(Dataset Loading)\n",
    "(2) 데이터 전처리(Data Preprocessing)\n",
    "   - 데이터 정규화(Normalization)\n",
    "   - 학습과 테스트 데이터 분할(Train/Test Data Split) 등\n",
    "(3) 학습 모델 구성(Train Model Build)\n",
    "(4) 학습(Model Training)\n",
    "(5) 학습 모델 성능 검증(Model Performance Validation)\n",
    "(6) 학습 모델 저장(배포) 하기(Model Save)\n",
    "(7) 추론 데이터 전처리((Data Preprocessing)\n",
    "(8) 추론(Inference) 또는 예측(Prediction) \n",
    "(9) 추론 결과 데이터 후처리(Data Postprocessing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 전처리 모듈 관리, 학습 알고리즘 관리 함수 설명\n",
    "\n",
    "1) [preprocess.py] 전처리모듈 관리 함수 \n",
    "\n",
    "def process_for_train(pm):\n",
    "    \"\"\"\n",
    "    (1) 입력: pm\n",
    "      # pm.source_path: 학습플랫폼/데이터셋 관리 메뉴에서 저장한 데이터를 불러오는 경로\n",
    "      # pm.target_path: 처리 완료된 데이터를 저장하는 경로\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # 데이터셋 관리 메뉴에서 저장한 데이터를 불러와서 필요한 처리를 수행\n",
    "      # 처리 완료된 데이터를 저장하는 기능, pm.target_path에 저장\n",
    "      # 실행환경 등록에서 General 선택: train() 함수의 T3QAI_TRAIN_DATA_PATH를 통해 데이터를 불러와서 전처리와 학습을 수행 \n",
    "    \"\"\"\n",
    "\n",
    "def init_svc(im, rule):\n",
    "    \"\"\"\n",
    "    (1) 입력: im, rule\n",
    "    (2) 출력: 전처리 객체를 딕셔너리(dictionary) 객체에 담아 리턴(return)\n",
    "    (3) 설명: \n",
    "      # process_for_train(pm) 함수에서 저장한 전처리 객체와 데이터에 적용된 룰(rule)을 불러오는 기능\n",
    "      # 전처리 객체, 룰(rule) 불러오기 기능 없이 처리\n",
    "    \"\"\"\n",
    "\n",
    "    return {}\n",
    "\n",
    "def transform(df, params, batch_id):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, params, batch_id\n",
    "      # df: 추론모델관리와 추론API관리, 실시간 추론을 통해 전달되는 추론 입력 데이터(dataframe 형태)\n",
    "      # params: init_svc(im, rule) 함수의 리턴(return) 값을 params 변수로 전달\n",
    "    (2) 출력: df\n",
    "    (3) 설명: \n",
    "      # df(추론 입력 데이터)에 대한 전처리를 수행한 후 전처리 된 데이터를 inference_dataframe(df, model_info_dict) 함수의 \n",
    "      입력 df에 전달하는 기능\n",
    "      # df(추론 입력 데이터)를 전처리 없이 inference_dataframe(df, model_info_dict) 함수의 입력 df에 리턴(return)\n",
    "    \"\"\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1) [train.py] 학습 알고리즘 관리 함수\n",
    "\n",
    "import t3qai_client as tc\n",
    "from t3qai_client import T3QAI_TRAIN_OUTPUT_PATH, T3QAI_TRAIN_MODEL_PATH, T3QAI_TRAIN_DATA_PATH\n",
    "\"\"\"\n",
    "(1) 설명:\n",
    "  # t3qai_client : 플랫폼과의 연동을 위한 클라이언트 모듈\n",
    "  # T3QAI_TRAIN_DATA_PATH : pm.target_path에서 저장한 전처리 데이터 경로\n",
    "  # T3QAI_TRAIN_MODEL_PATH : 학습 모델 저장 경로\n",
    "  # T3QAI_TRAIN_OUTPUT_PATH : 학습 결과 출력파일 저장 경로\n",
    "\"\"\"\n",
    "      \n",
    "def train():\n",
    "    \"\"\"\n",
    "    (1) 입력: None\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # pm.target_path에 저장한 데이터를 T3QAI_TRAIN_DATA_PATH 에서 불러오기\n",
    "      # 데이터 전처리와 학습 모델을 구성하고 모델 학습을 수행\n",
    "      # 학습 모델의 성능을 검증하고 배포할 학습 모델을 저장\n",
    "      # 전처리 객체와 학습 모델 객체를 T3QAI_TRAIN_MODEL_PATH 에 저장\n",
    "      # 학습 결과를 파일(이미지, 텍스트 등) 형태로 T3QAI_TRAIN_OUTPUT_PATH 에 저장 \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2) [inference_service.py] 학습 알고리즘 관리 함수\n",
    "\n",
    "import t3qai_client as tc\n",
    "from t3qai_client import T3QAI_INIT_MODEL_PATH\n",
    "\"\"\"\n",
    "(1) 설명:\n",
    "  # T3QAI_INIT_MODEL_PATH : train() 함수에서 T3QAI_TRAIN_MODEL_PATH 에 저장한 전처리 객체와 \n",
    "                            학습 모델 객체 등을 추론 하기 위해 불러오는 경로\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"\n",
    "    (1) 입력: None\n",
    "    (2) 출력: 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 객체에 담아 리턴(return)\n",
    "    (3) 설명: \n",
    "      # T3QAI_TRAIN_MODEL_PATH에 저장한 전처리 객체와 학습 모델 객체 등을 불러오는 기능\n",
    "      # 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "      # 리턴(return) 값을 inference_dataframe(df,model_info_dict), \n",
    "      inference_file(files, model_info_dict) 함수의 입력 model_info_dict 변수로 전달\n",
    "    \"\"\"\n",
    "    return { **params }\n",
    "\n",
    "def inference_dataframe(df, model_info_dict):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, model_info_dict\n",
    "      # df: transform(df, params, batch_id)함수의 리턴(return) 값으로 전달된 df, \n",
    "      추론 입력 데이터(dataframe 형태)\n",
    "      # model_info_dict: init_model() 함수의 return 값을 model_info_dict 변수로 전달\n",
    "        ## 학습 모델 객체 사용 예시       model = model_info_dict.get('model') 또는 \n",
    "                                          model = model_info_dict['model']\n",
    "        ## 전처리(pca) 객체 사용 예시     pca = model_info_dict.get['pca'] 또는\n",
    "                                          pca = model_info_dict['pca']\n",
    "                                          \n",
    "    (2) 출력: 추론 결과 딕셔너리(dictionary) 형태 \n",
    "            result = {'inference': inference_result}\n",
    "\n",
    "                            \n",
    "    (3) 설명: \n",
    "      # 전처리 객체를 사용하여 df(추론 입력 데이터)에 대한 전처리 수행\n",
    "      # 배포된 학습 모델(model)을 사용하여 df(추론 입력 데이터)에 대한 추론(예측)을 수행\n",
    "      # 추론 결과를 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    \"\"\"\n",
    "    return {**result}\n",
    "\n",
    "def inference_file(files, model_info_dict):\n",
    "    \"\"\"\n",
    "    (1) 입력: files, model_info_dict\n",
    "      # files: 추론 하고자 하는 파일 형태의 입력 \n",
    "      # model_info_dict: init_model() 함수의 return 값을 model_info_dict 변수로 전달\n",
    "        ## 학습 모델 객체 사용 예시       model = model_info_dict.get('model') 또는 \n",
    "                                          model = model_info_dict['model']\n",
    "        ## 전처리(pca) 객체 사용 예시     pca = model_info_dict.get['pca'] 또는\n",
    "                                          pca = model_info_dict['pca']\n",
    "        \n",
    "    (2) 출력: a. 추론 결과 딕셔너리(dictionary) 형태 \n",
    "                  result = {'inference': inference_result}\n",
    "              b. 추론 결과 DownloadFile 형태\n",
    "                  result = DownloadFile(file_path=resultfilepath, file_name=filename1)\n",
    "                  result = DownloadFile(file_obj=resultfileobj, file_name=filename2)\n",
    "              c. 추론 결과 DownloadFile의 list형태\n",
    "                  result = [DownloadFile(file_path=resultfilepath, file_name=filename), \n",
    "                            DownloadFile(file_obj=resultfileobj, file_name=filename), ...]\n",
    "              \n",
    "    (3) 설명: \n",
    "      # 전처리 객체를 사용하여 files(추론 입력 데이터)에 대한 전처리 수행\n",
    "      # 배포된 학습 모델(model)을 사용하여 files(추론 입력 데이터)에 추론(예측)을 수행\n",
    "      # 추론 결과를 a.딕셔너리(dictionary) 형태, b.DownloadFile 형태, c.DownloadFile의 list 형태로 리턴(return)\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "\n",
    "from preprocess_sub_seg import exec_process\n",
    "import logging\n",
    "\n",
    "def process_for_train(pm):\n",
    "    exec_process(pm)\n",
    "    logging.info('[hunmin log] the end line of the function [process_for_train]')\n",
    "    \n",
    "def init_svc(im, rule):\n",
    "    return {}\n",
    "\n",
    "def transform(df, params, batch_id):\n",
    "    logging.info('[hunmin log] df.shape : {}'.format(df.shape))\n",
    "    logging.info('[hunmin log] type(df) : {}'.format(type(df)))\n",
    "    logging.info('[hunmin log] the end line of the function [transform]')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess_sub.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import logging\n",
    "\n",
    "\n",
    "def exec_process(pm):\n",
    "    logging.info('[hunmin log] the start line of the function [exec_process]')\n",
    "\n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(pm.source_path)\n",
    "    \n",
    "    # pm.source_path의 dataset.zip 파일을\n",
    "    # pm.target_path 경로에 압축해제\n",
    "    my_zip_path = os.path.join(pm.source_path,'./meta_data.zip')\n",
    "    extract_zip_file = zipfile.ZipFile(my_zip_path)\n",
    "    extract_zip_file.extractall(pm.target_path)\n",
    "    extract_zip_file.close()\n",
    "    \n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(pm.target_path)\n",
    "\n",
    "    logging.info('[hunmin log] the finish line of the function [exec_process]')\n",
    "\n",
    "# 저장 파일 확인\n",
    "def list_files_directories(path):\n",
    "    # Get the list of all files and directories in current working directory\n",
    "    dir_list = os.listdir(path)\n",
    "    logging.info('[hunmin log] Files and directories in {} :'.format(path))\n",
    "    logging.info('[hunmin log] dir_list : {}'.format(dir_list))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T3QAI_TRAIN_OUTPUT_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "T3QAI_TRAIN_MODEL_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "T3QAI_TRAIN_DATA_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "T3QAI_TEST_DATA_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "T3QAI_MODULE_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "T3QAI_INIT_MODEL_PATH: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data\n",
      "df:                                                     0\n",
      "0  iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAA...\n",
      "df.dtypes: 0    object\n",
      "dtype: object\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "파일 패스\n",
      "/Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/.venv/lib/python3.10/site-packages/ultralytics/utils/__init__.py\n",
      "루트\n",
      "/Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/.venv/lib/python3.10/site-packages/ultralytics\n",
      "디폹트 CFG 경로\n",
      "/Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/.venv/lib/python3.10/site-packages/ultralytics/cfg/default.yaml\n",
      "야믈 파일 읽기 결과1\n",
      "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
      "# Default training settings and hyperparameters for medium-augmentation COCO training\n",
      "\n",
      "task: detect # (str) YOLO task, i.e. detect, segment, classify, pose\n",
      "mode: train # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark\n",
      "\n",
      "# Train settings -------------------------------------------------------------------------------------------------------\n",
      "model: # (str, optional) path to model file, i.e. yolov8n.pt, yolov8n.yaml\n",
      "data: # (str, optional) path to data file, i.e. coco8.yaml\n",
      "epochs: 100 # (int) number of epochs to train for\n",
      "time: # (float, optional) number of hours to train for, overrides epochs if supplied\n",
      "patience: 100 # (int) epochs to wait for no observable improvement for early stopping of training\n",
      "batch: 16 # (int) number of images per batch (-1 for AutoBatch)\n",
      "imgsz: 640 # (int | list) input images size as int for train and val modes, or list[h,w] for predict and export modes\n",
      "save: True # (bool) save train checkpoints and predict results\n",
      "save_period: -1 # (int) Save checkpoint every x epochs (disabled if < 1)\n",
      "cache: False # (bool) True/ram, disk or False. Use cache for data loading\n",
      "device: # (int | str | list, optional) device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu\n",
      "workers: 8 # (int) number of worker threads for data loading (per RANK if DDP)\n",
      "project: # (str, optional) project name\n",
      "name: # (str, optional) experiment name, results saved to 'project/name' directory\n",
      "exist_ok: False # (bool) whether to overwrite existing experiment\n",
      "pretrained: True # (bool | str) whether to use a pretrained model (bool) or a model to load weights from (str)\n",
      "optimizer: auto # (str) optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
      "verbose: True # (bool) whether to print verbose output\n",
      "seed: 0 # (int) random seed for reproducibility\n",
      "deterministic: True # (bool) whether to enable deterministic mode\n",
      "single_cls: False # (bool) train multi-class data as single-class\n",
      "rect: False # (bool) rectangular training if mode='train' or rectangular validation if mode='val'\n",
      "cos_lr: False # (bool) use cosine learning rate scheduler\n",
      "close_mosaic: 10 # (int) disable mosaic augmentation for final epochs (0 to disable)\n",
      "resume: False # (bool) resume training from last checkpoint\n",
      "amp: True # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check\n",
      "fraction: 1.0 # (float) dataset fraction to train on (default is 1.0, all images in train set)\n",
      "profile: False # (bool) profile ONNX and TensorRT speeds during training for loggers\n",
      "freeze: None # (int | list, optional) freeze first n layers, or freeze list of layer indices during training\n",
      "multi_scale: False # (bool) Whether to use multiscale during training\n",
      "# Segmentation\n",
      "overlap_mask: True # (bool) masks should overlap during training (segment train only)\n",
      "mask_ratio: 4 # (int) mask downsample ratio (segment train only)\n",
      "# Classification\n",
      "dropout: 0.0 # (float) use dropout regularization (classify train only)\n",
      "\n",
      "# Val/Test settings ----------------------------------------------------------------------------------------------------\n",
      "val: True # (bool) validate/test during training\n",
      "split: val # (str) dataset split to use for validation, i.e. 'val', 'test' or 'train'\n",
      "save_json: False # (bool) save results to JSON file\n",
      "save_hybrid: False # (bool) save hybrid version of labels (labels + additional predictions)\n",
      "conf: # (float, optional) object confidence threshold for detection (default 0.25 predict, 0.001 val)\n",
      "iou: 0.7 # (float) intersection over union (IoU) threshold for NMS\n",
      "max_det: 300 # (int) maximum number of detections per image\n",
      "half: False # (bool) use half precision (FP16)\n",
      "dnn: False # (bool) use OpenCV DNN for ONNX inference\n",
      "plots: True # (bool) save plots and images during train/val\n",
      "\n",
      "# Predict settings -----------------------------------------------------------------------------------------------------\n",
      "source: # (str, optional) source directory for images or videos\n",
      "vid_stride: 1 # (int) video frame-rate stride\n",
      "stream_buffer: False # (bool) buffer all streaming frames (True) or return the most recent frame (False)\n",
      "visualize: False # (bool) visualize model features\n",
      "augment: False # (bool) apply image augmentation to prediction sources\n",
      "agnostic_nms: False # (bool) class-agnostic NMS\n",
      "classes: # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]\n",
      "retina_masks: False # (bool) use high-resolution segmentation masks\n",
      "embed: # (list[int], optional) return feature vectors/embeddings from given layers\n",
      "\n",
      "# Visualize settings ---------------------------------------------------------------------------------------------------\n",
      "show: False # (bool) show predicted images and videos if environment allows\n",
      "save_frames: False # (bool) save predicted individual video frames\n",
      "save_txt: False # (bool) save results as .txt file\n",
      "save_conf: False # (bool) save results with confidence scores\n",
      "save_crop: False # (bool) save cropped images with results\n",
      "show_labels: True # (bool) show prediction labels, i.e. 'person'\n",
      "show_conf: True # (bool) show prediction confidence, i.e. '0.99'\n",
      "show_boxes: True # (bool) show prediction boxes\n",
      "line_width: # (int, optional) line width of the bounding boxes. Scaled to image size if None.\n",
      "\n",
      "# Export settings ------------------------------------------------------------------------------------------------------\n",
      "format: torchscript # (str) format to export to, choices at https://docs.ultralytics.com/modes/export/#export-formats\n",
      "keras: False # (bool) use Kera=s\n",
      "optimize: False # (bool) TorchScript: optimize for mobile\n",
      "int8: False # (bool) CoreML/TF INT8 quantization\n",
      "dynamic: False # (bool) ONNX/TF/TensorRT: dynamic axes\n",
      "simplify: False # (bool) ONNX: simplify model using `onnxslim`\n",
      "opset: # (int, optional) ONNX: opset version\n",
      "workspace: 4 # (int) TensorRT: workspace size (GB)\n",
      "nms: False # (bool) CoreML: add NMS\n",
      "\n",
      "# Hyperparameters ------------------------------------------------------------------------------------------------------\n",
      "lr0: 0.01 # (float) initial learning rate (i.e. SGD=1E-2, Adam=1E-3)\n",
      "lrf: 0.01 # (float) final learning rate (lr0 * lrf)\n",
      "momentum: 0.937 # (float) SGD momentum/Adam beta1\n",
      "weight_decay: 0.0005 # (float) optimizer weight decay 5e-4\n",
      "warmup_epochs: 3.0 # (float) warmup epochs (fractions ok)\n",
      "warmup_momentum: 0.8 # (float) warmup initial momentum\n",
      "warmup_bias_lr: 0.1 # (float) warmup initial bias lr\n",
      "box: 7.5 # (float) box loss gain\n",
      "cls: 0.5 # (float) cls loss gain (scale with pixels)\n",
      "dfl: 1.5 # (float) dfl loss gain\n",
      "pose: 12.0 # (float) pose loss gain\n",
      "kobj: 1.0 # (float) keypoint obj loss gain\n",
      "label_smoothing: 0.0 # (float) label smoothing (fraction)\n",
      "nbs: 64 # (int) nominal batch size\n",
      "hsv_h: 0.015 # (float) image HSV-Hue augmentation (fraction)\n",
      "hsv_s: 0.7 # (float) image HSV-Saturation augmentation (fraction)\n",
      "hsv_v: 0.4 # (float) image HSV-Value augmentation (fraction)\n",
      "degrees: 0.0 # (float) image rotation (+/- deg)\n",
      "translate: 0.1 # (float) image translation (+/- fraction)\n",
      "scale: 0.5 # (float) image scale (+/- gain)\n",
      "shear: 0.0 # (float) image shear (+/- deg)\n",
      "perspective: 0.0 # (float) image perspective (+/- fraction), range 0-0.001\n",
      "flipud: 0.0 # (float) image flip up-down (probability)\n",
      "fliplr: 0.5 # (float) image flip left-right (probability)\n",
      "bgr: 0.0 # (float) image channel BGR (probability)\n",
      "mosaic: 1.0 # (float) image mosaic (probability)\n",
      "mixup: 0.0 # (float) image mixup (probability)\n",
      "copy_paste: 0.0 # (float) segment copy-paste (probability)\n",
      "auto_augment: randaugment # (str) auto augmentation policy for classification (randaugment, autoaugment, augmix)\n",
      "erasing: 0.4 # (float) probability of random erasing during classification training (0-0.9), 0 means no erasing, must be less than 1.0.\n",
      "crop_fraction: 1.0 # (float) image crop fraction for classification (0.1-1), 1.0 means no crop, must be greater than 0.\n",
      "\n",
      "# Custom config.yaml ---------------------------------------------------------------------------------------------------\n",
      "cfg: # (str, optional) for overriding defaults.yaml\n",
      "\n",
      "# Tracker settings ------------------------------------------------------------------------------------------------------\n",
      "tracker: botsort.yaml # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]\n",
      "\n",
      "야믈 파일 읽기 결과\n",
      "{'task': 'detect', 'mode': 'train', 'model': None, 'data': None, 'epochs': 100, 'time': None, 'patience': 100, 'batch': 16, 'imgsz': 640, 'save': True, 'save_period': -1, 'cache': False, 'device': None, 'workers': 8, 'project': None, 'name': None, 'exist_ok': False, 'pretrained': True, 'optimizer': 'auto', 'verbose': True, 'seed': 0, 'deterministic': True, 'single_cls': False, 'rect': False, 'cos_lr': False, 'close_mosaic': 10, 'resume': False, 'amp': True, 'fraction': 1.0, 'profile': False, 'freeze': 'None', 'multi_scale': False, 'overlap_mask': True, 'mask_ratio': 4, 'dropout': 0.0, 'val': True, 'split': 'val', 'save_json': False, 'save_hybrid': False, 'conf': None, 'iou': 0.7, 'max_det': 300, 'half': False, 'dnn': False, 'plots': True, 'source': None, 'vid_stride': 1, 'stream_buffer': False, 'visualize': False, 'augment': False, 'agnostic_nms': False, 'classes': None, 'retina_masks': False, 'embed': None, 'show': False, 'save_frames': False, 'save_txt': False, 'save_conf': False, 'save_crop': False, 'show_labels': True, 'show_conf': True, 'show_boxes': True, 'line_width': None, 'format': 'torchscript', 'keras': False, 'optimize': False, 'int8': False, 'dynamic': False, 'simplify': False, 'opset': None, 'workspace': 4, 'nms': False, 'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'pose': 12.0, 'kobj': 1.0, 'label_smoothing': 0.0, 'nbs': 64, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'copy_paste': 0.0, 'auto_augment': 'randaugment', 'erasing': 0.4, 'crop_fraction': 1.0, 'cfg': None, 'tracker': 'botsort.yaml'}\n",
      "야믈 파일 읽기 결과1\n",
      "settings_version: 0.0.4\n",
      "datasets_dir: /Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data/dataset/od\n",
      "weights_dir: weights\n",
      "runs_dir: runs\n",
      "uuid: 816a5e1f1ba3ecf33897edc174448127a005dd2e97fde3f240e1adedc5b407bd\n",
      "sync: true\n",
      "api_key: ''\n",
      "clearml: true\n",
      "comet: true\n",
      "dvc: true\n",
      "hub: true\n",
      "mlflow: true\n",
      "neptune: true\n",
      "raytune: true\n",
      "tensorboard: true\n",
      "wandb: true\n",
      "\n",
      "야믈 파일 읽기 결과\n",
      "{'settings_version': '0.0.4', 'datasets_dir': '/Users/myoungjikim/2024_inisw4_IPRGS_t3q.dl/meta_data/dataset/od', 'weights_dir': 'weights', 'runs_dir': 'runs', 'uuid': '816a5e1f1ba3ecf33897edc174448127a005dd2e97fde3f240e1adedc5b407bd', 'sync': True, 'api_key': '', 'clearml': True, 'comet': True, 'dvc': True, 'hub': True, 'mlflow': True, 'neptune': True, 'raytune': True, 'tensorboard': True, 'wandb': True}\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import logging\n",
    "import train\n",
    "import t3qai_client as tc\n",
    "import train_sub_lp as np\n",
    "import train_sub_od as od\n",
    "\n",
    "\n",
    "def main():\n",
    "    result = None\n",
    "    result_msg = \"success\"\n",
    "    a = tc.t3qai_client()\n",
    "    a.train_start()\n",
    "    try:\n",
    "        train()\n",
    "    except Exception as e:\n",
    "        result = e\n",
    "        result_msg = e\n",
    "        logging.info('error log : {}'.format(e))\n",
    "    a.train_finish(result, result_msg)\n",
    "\n",
    "def train():\n",
    "    logging.info('[hunmin log] the start line of the function [train]')\n",
    "    od.exec_train()\n",
    "    np.exec_train()\n",
    "    logging.info('[hunmin log] the end line of the function [train]')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "detach",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minference_service_sub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exec_inference_dataframe\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_service_sub_od\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mod_inference\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_service_sub_seg\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mseg_inference\u001b[39;00m\n",
      "File \u001b[0;32m~/2024_inisw4_IPRGS_t3q.dl/inference_service_sub.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_service_sub_od\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mod_inference\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_service_sub_seg\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mseg_inference\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_service_sub_lp\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlp_inference\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marea\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01marea\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexec_inference_dataframe\u001b[39m(df, models_info_dict):\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# SEG MODEL\u001b[39;00m\n",
      "File \u001b[0;32m~/2024_inisw4_IPRGS_t3q.dl/inference_service_sub_lp.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minference_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlicense_plate\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 모델 클래스\u001b[39;00m\n",
      "File \u001b[0;32m~/2024_inisw4_IPRGS_t3q.dl/inference_utils/license_plate.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(output, cropped_car, seg_image, car_bbox, reader):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output:\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: detach"
     ]
    }
   ],
   "source": [
    "# inference_service\n",
    "\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "from inference_service_sub import exec_inference_dataframe\n",
    "import inference_service_sub_od as od_inference\n",
    "import inference_service_sub_seg as seg_inference\n",
    "import inference_service_sub_lp as lp_inference\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "# 모델 초기화\n",
    "def init_model():\n",
    "    logging.info('[hunmin log] the start line of the function [init_model]')\n",
    "    od_params = od_inference.exec_init_model()\n",
    "    seg_params = seg_inference.exec_init_model()\n",
    "    lp_params = lp_inference.exec_init_model()\n",
    "    logging.info('[hunmin log] the end line of the function [init_model]')\n",
    "    return {\"od_params\":{ **od_params },\"seg_params\":{ **seg_params },\"lp_params\":{ **lp_params }}\n",
    "\n",
    "# 모델 추론 - dataframe\n",
    "def inference_dataframe(df, models_info_dict):\n",
    "    logging.info(f'[hunmin log] the start line of the function [inference_dataframe]')\n",
    "\n",
    "    final_image, od_result, area_output, license_number, full_od_result, error = exec_inference_dataframe(df, models_info_dict)\n",
    "    response_data = {\n",
    "        'msg': error if error else \"success\",\n",
    "        'image': pil_image_to_base64(final_image) if final_image else None,\n",
    "        'od_result': od_result if od_result else [],\n",
    "        'area': area_output if area_output else {},\n",
    "        'license_number': license_number if license_number else \"\"\n",
    "    }\n",
    "    logging.info(f'[hunmin log] the end line of the function [inference_dataframe].')\n",
    "    return response_data\n",
    "\n",
    "def pil_image_to_base64(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "# 모델 추론 - file\n",
    "# def inference_file(files, models_info_dict):\n",
    "#     od_result = od_inference.exec_inference_file(files, models_info_dict['od_params'])\n",
    "#     lp_result = lp_inference.exec_inference_file(files, models_info_dict['lp_params'])\n",
    "#     seg_result = seg_inference.exec_inference_file(files, models_info_dict['seg_params'])\n",
    "#     result = {'od_result':od_result,'seg_result':seg_result,'lp_result':lp_result}\n",
    "#     logging.info('[hunmin log] the end line of the function [inference_file]')\n",
    "#     return result\n",
    "\n",
    "# 추론 실행\n",
    "# def main():\n",
    "\n",
    "#     # 모델 초기화\n",
    "#     models_info_dict = init_model()\n",
    "\n",
    "#     # 테스트 데이터셋 준비\n",
    "#     image = Image.open('./meta_data/dataset/test/Cars122.jpeg')\n",
    "#     base64_image = pil_image_to_base64(image)\n",
    "#     with open('./temp_base64_image', '+w' ) as file:\n",
    "#         file.write(base64_image)\n",
    "#     image_data = [[base64_image]]\n",
    "#     df = pd.DataFrame(image_data)\n",
    "\n",
    "#     # 모델 추론 요청\n",
    "#     result = inference_dataframe(df, models_info_dict)\n",
    "\n",
    "#     with open('./temp_result.txt', '+w') as file:\n",
    "#         file.write(str(result))\n",
    "\n",
    "#     # Convert bytes data to an image\n",
    "#     base64_string = result['image']\n",
    "#     decoded_image = base64.b64decode(base64_string)\n",
    "#     result_image = Image.open(BytesIO(decoded_image))\n",
    "\n",
    "#     # Save the image as a PNG file\n",
    "#     result_image.save('final_image.png')\n",
    "\n",
    "#     logging.info('모델 추론 결과')\n",
    "#     logging.info(f'msg: {result[\"msg\"]}')\n",
    "#     logging.info(f'od_result: {result[\"od_result\"]}')\n",
    "#     logging.info(f'area: {result[\"area\"]}')\n",
    "#     logging.info(f'license_number: {result[\"license_number\"]}')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_service_sub\n",
    "\n",
    "import logging, os\n",
    "from ultralytics import YOLO\n",
    "from t3qai_client import T3QAI_INIT_MODEL_PATH, T3QAI_TRAIN_MODEL_PATH\n",
    "\n",
    "import inference_service_sub_od as od_inference\n",
    "import inference_service_sub_seg as seg_inference\n",
    "import inference_service_sub_lp as lp_inference\n",
    "import meta_data.postprocess_utils.area as area\n",
    "\n",
    "\n",
    "def exec_inference_dataframe(df, models_info_dict):\n",
    "\n",
    "    # SEG MODEL\n",
    "    try:\n",
    "        seg_image, areas, car_bbox, seg_result, seg_error = seg_inference.exec_inference_dataframe(df, models_info_dict['seg_params'])\n",
    "\n",
    "        logging.info(f'seg_model inference result: { {\"seg_image\": type(seg_image), \"areas\": areas, \"car_bbox\":car_bbox, \"seg_result\": seg_result, \"seg_error\":seg_error}}')\n",
    "\n",
    "        if seg_error:\n",
    "            return None, None, None, None, None, seg_error\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, None, None, None, None, str(e) + \" (in SEG)\"\n",
    "\n",
    "    # AREA MODEL\n",
    "    try:\n",
    "        area_output = area.process(areas)\n",
    "        logging.info(f'area process result: { {\"area_ouput\":area_output}}')\n",
    "\n",
    "    except Exception as e:\n",
    "        return seg_image, seg_result, None, None, None, str(e) + \" (in AREA)\"\n",
    "\n",
    "    # LP MODEL\n",
    "    try:\n",
    "        seg_lp_image, license_number, lp_error = lp_inference.exec_inference_dataframe(df, car_bbox, seg_image, models_info_dict['lp_params'])\n",
    "        logging.info(f'LP MODEL process result: { {\"seg_lp_image\":type(seg_lp_image), \"license_number\": license_number, \"lp_error\":lp_error}}')\n",
    "\n",
    "        if lp_error:\n",
    "            return seg_image, seg_result, area_output, None, None, lp_error  + \" (in LP)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return seg_image, seg_result, area_output, None, None, str(e)  + \" (in LP)\"\n",
    "\n",
    "    # OD MODEL\n",
    "    try:\n",
    "        final_image, od_result, full_od_result, od_error = od_inference.exec_inference_dataframe(df,seg_image,models_info_dict['od_params'])\n",
    "        logging.info(f'OD MODEL process result: { {\"final_image\":type(final_image), \"od_result\": od_result, \"full_od_result\":full_od_result, \"od_error\":od_error}}')\n",
    "\n",
    "        if od_error:\n",
    "            return seg_lp_image, seg_result, area_output, license_number, None, od_error + \" (in OD)\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return seg_lp_image, seg_result, area_output, license_number, None, str(e) + \" (in OD)\"\n",
    "\n",
    "    # result\n",
    "    return final_image, seg_result + od_result, area_output, license_number, full_od_result, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t3q_client\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import FileUpload\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# t3qai_client 클래스: t3qai_client 객체\n",
    "class t3qai_client:\n",
    "    def train_start(self):\n",
    "        return None\n",
    "\n",
    "    def train_finish(self, result, result_msg):\n",
    "        if result_msg != \"success\":\n",
    "            raise Exception(result_msg)\n",
    "        else:\n",
    "            logging.info(result)\n",
    "            logging.info(\"train finish\")\n",
    "\n",
    "    def train_load_param(self):\n",
    "        '''set_param'''\n",
    "        epoch = 20\n",
    "        batch_size = 16\n",
    "        params = {\"epoch\" : epoch, 'batch_size' : batch_size}\n",
    "        return { **params }\n",
    "\n",
    "class PM:\n",
    "    def __init__(self):\n",
    "        self.source_path = './'\n",
    "        self.target_path = './meta_data'\n",
    "        \n",
    "class UploadFile:\n",
    "    def __init__(self, file, filename):\n",
    "        self.file = file\n",
    "        self.filename = filename\n",
    "\n",
    "def DownloadFile(file_name, file_obj = None, file_path = None):\n",
    "    file_route = './meta_data/DownloadFiles'\n",
    "    os.makedirs(file_route, exist_ok = True)\n",
    "    file_dir = os.path.join(file_route, file_name)\n",
    "    if (file_obj == None) == (file_path == None):\n",
    "        Err_msg = \"[DownloadFile Error]: Only one of the 'file_path' or 'file_obj' arguments is required.\"\n",
    "        Err_msg += f\"{0 if file_obj==None else 2} arguments entered.\"\n",
    "        raise Exception(Err_msg)\n",
    "    elif(file_obj != None):\n",
    "        file_obj.seek(0)\n",
    "        file_read = base64.b64encode(file_obj.read()).decode('utf-8')\n",
    "        binary_file = base64.b64decode(file_read)\n",
    "        with open(file_dir, 'wb') as f:\n",
    "            f.write(binary_file)\n",
    "    elif(file_path != None):\n",
    "        shutil.copyfile(file_path, file_dir)\n",
    "        \n",
    "    return FileLink(file_dir)\n",
    "\n",
    "pm = PM()\n",
    "\n",
    "T3QAI_TRAIN_OUTPUT_PATH = './meta_data'\n",
    "T3QAI_TRAIN_MODEL_PATH = './meta_data'\n",
    "T3QAI_TRAIN_DATA_PATH = './meta_data'\n",
    "T3QAI_TEST_DATA_PATH = './meta_data'\n",
    "T3QAI_MODULE_PATH = './meta_data'\n",
    "T3QAI_INIT_MODEL_PATH = './meta_data'\n",
    "\n",
    "\n",
    "# t3qai_client 객체\n",
    "tc = t3qai_client()\n",
    "print('T3QAI_TRAIN_OUTPUT_PATH:', T3QAI_TRAIN_OUTPUT_PATH)\n",
    "print('T3QAI_TRAIN_MODEL_PATH:', T3QAI_TRAIN_MODEL_PATH)\n",
    "print('T3QAI_TRAIN_DATA_PATH:', T3QAI_TRAIN_DATA_PATH)\n",
    "print('T3QAI_TEST_DATA_PATH:', T3QAI_TEST_DATA_PATH)\n",
    "print('T3QAI_MODULE_PATH:', T3QAI_MODULE_PATH)\n",
    "print('T3QAI_INIT_MODEL_PATH:', T3QAI_INIT_MODEL_PATH)\n",
    "\n",
    "# init_svc(im, rule) 함수 입력\n",
    "im = None\n",
    "rule = None\n",
    "# transform(df, params, batch_id) 함수 입력\n",
    "batch_id = 0\n",
    "\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# base64 encoded image - apple.jpg\n",
    "data = [['iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAACySURBVEhL7ZLRDoAgCEWt//9nc8EIEepStvXQeWkyPEKw1FrLbFb+TuWXzichXXb4cAokJR0tH+JFK21G8V6CSqlApMxGelBIsVBHeOOEzZYGdRzpussfL7eIazHPa0wrx0GMdBSiuMbkdINyb5og0gRL3dTbcPtNOpYZveQ2pA1n0hTakF5+hA9Lzd87pNFYLhkvsvT2lMhorndluxkRUuCYbzcp9ROi55+up8sLK1XKBj1wbx3DelAOAAAAAElFTkSuQmCC']]\n",
    "df = pd.DataFrame(data)\n",
    "print('df: ', df)\n",
    "print('df.dtypes:', df.dtypes)\n",
    "\n",
    "# inference_file 함수 추론\n",
    "files = []\n",
    "\n",
    "uploader = FileUpload(accept='*', multiple=True, description='select data', button_style='danger')\n",
    "def uploader_change(change):\n",
    "    uploader.button_style='success'\n",
    "    count = len(uploader.value)\n",
    "    uploader._counter = count\n",
    "    files.clear()\n",
    "    for file_num in range(count):\n",
    "        temp_data = tempfile.TemporaryFile()\n",
    "        if ipywidgets.__version__[0] == '7':\n",
    "            temp_data.write(list(uploader.value.values())[file_num]['content'])\n",
    "            file = UploadFile(temp_data, pd.DataFrame(list(uploader.value.values())[file_num]).iloc[1,0])\n",
    "        elif int(ipywidgets.__version__[0]) > 7:\n",
    "            temp_data.write(uploader.value[file_num].content)\n",
    "            file = UploadFile(temp_data, uploader.value[file_num].name)\n",
    "        files.append(file)\n",
    "\n",
    "uploader.observe(uploader_change, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_for_train(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = init_svc(im, rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = transform(df, params, batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models_info_dict = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inference_dataframe(df, models_info_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
